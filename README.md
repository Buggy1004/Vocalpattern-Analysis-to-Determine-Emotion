#Preview
Human when communicate they use emotion to convey their message to each other  clearly so User can say that emotion is equally important as the content said by humans. The  role of emotion in the context which defines the message led to the demand of emotion  recognition system and expansion that idea led to demand of human computer interaction  system. The advances that User made in Machine learning field lead by many researchers and  the requirement of real time speech emotion recognition system for the human computer  interaction led to the creation of many datasets and methods to achieve and better the accuracy  or existing solution for emotion recognition in speech. Human emotion is very complex in  nature and due to which, what feature User select can have major impact on our machine  learning model outcome. This Project is to explore many machine learning methods and feature  extraction methods for creation of speech emotion recognition system. 
The potentiality to vary vocal sounds in order to produce speech is one of the major  features which sets humans apart from other living beings. User can categorize human emotion  by several attributes such as pitch, timbre, loudness, and vocal tone. It has often been noticed  that humans express their emotions by varying different vocal attributes during speech  generation. Hence, identification of human emotions using voice and speech analysis has a  practical possibility and could potentially be beneficial in improving human communicational skills. One can follow an algorithmic approach for detection and analysis of human emotions  with the help of voice and speech processing. The proposed approach has been developed with  the objective of incorporation with futuristic machine learning systems for improving human 
computer interactions with the help of machine learning models SVM (Support Vector  Machine) and CNN (Convolution Neural Network) by the extraction of MFCC features. The  above-mentioned models are trained using RAVDESS and TESS datasets.
A great deal of multidisciplinary study have been done which Usernt into understanding  speech emotion detection throughout the years. Researchers in the neuroscience field study  how the brain receives incoming stimuli by analysing raw audio data. Linguists would rather  observe speech that can supply emotional information through semantic and syntactic analysis  of the speech, but computational intelligence researchers provide tools to explain the  knowledge received from neuroscientists in a mathematical solution. These multidisciplinary  research collaborations have shoUserd enormous promise in comprehending voice emotion. 
The starting of SER(Structural Engineer of Record) was from psychology not from computer  science field as some scientist researched the role of acoustics of human emotion, that is what  is the effect of emotion upon the voice which is understood by other people.The idea for letting  computer understand the emotion was lead two decades before at the same time when machine  learning was also being popular, and many people was discovering many new machine learning  methods. Now after two decade of research the researchers can say that speech emotion  recognition is matured enough so that major advancement can be done in this area. In todayâ€™s  world there are lot data created by humans which can help us to create good dataset which can  used to train model in deep learning. Our report is based on speech emotion recognition that is  recognition of emotion by computer and to put it more accurately can say that enabling  computers to recognise emotion from the acoustic characteristics from the speech like pitch,  tone, loudness, and spectral distribution of frequency. HoUserver, recognising emotions from  speech is a difficult process. First and foremost, even for the same feeling, there is unlimited  diversity in emotional responses inside and across speakers. The diversity of emotions is  another aspect. People may shade or conceal their emotions because they are complex or  because of societal factors. Information must be distinguished from other impacts on the voice,  such as vocal abnormalities, breathing sound and physical ambiance sound. 
There is also a high demand for Voice based system, a computer system which uses  intelligence system to mimic emotion and read the content. For computer to understand the  emotion and to mimic is quite hard to build so much research is going on in this field which  can be also helper or reinforced by emotion recognition system. Speech emotion detection is  challenging due to the nature of emotion and complexity build by people around the world and  their culture. Which is solved by using machine learning method to classify the emotion in  speech. What has been accomplished thus far is the recognition of clearly expressed performed  emotions with excellent accuracy, but still real-world dataset is not created and still are not able
to identify emotion in real time in social environment. Prediction accuracies are based not only  on emotion types, but also on the number of emotion classes: from performed emotion, up to  eight emotion classes have been effectively identified, but from natural emotions, only five  classes can be meaningfully categorized, and even these with accuracies below 50%. The  proposed approach has been proven to work on both men and women as Userll as children.  HoUserver, thus far, technology has mostly been used in laboratory settings with a peaceful  background and a solitary clearly notice the loudness. 
#Objective 
The new era of automation has begun as a result of the increasing growth and  development in the fields of AI and machine learning. The majority of these automated gadgets  are controlled by the user's vocal commands. Many advantages can be created over present  systems if, in addition to identifying words, the machines can interpret the speaker's emotion  (user). Computer-based tutorial applications, automated contact center dialogues, a diagnostic  tool for therapy, and an automatic translation system are some of the applications of a speech  emotion detection system. The processes for creating a speech emotion recognition system  Userre addressed in detail in this thesis, and various tests Userre conducted to understand the  influence of each step.  
The low amount of publicly available speech databases made it difficult at first. The  processes for creating a speech emotion recognition system Userre addressed in detail in this  thesis, and various tests Userre conducted to understand the influence of each step. It was  initially difficult to create a Userll-trained model due to the low amount of publicly available  speech databases. Then, in previous research, multiple unique ways to feature extraction had  been offered, and identifying the optimal approach required doing numerous experiments.  Finally, learning about the strengths and Useraknesses of each classifying algorithm in terms  of emotion recognition was part of the classifier selection process. When comparing a single  feature to an integrated feature space, the results show that an integrated feature space produces  a higher recognition rate. 
Deep convolutional architectures that can learn from spectrogram representations of  speech are becoming increasingly popular. They, along with recurrent networks, are thought to  be a solid foundation for SER systems are a type of SER system. Over time, increasingly  complicated SER designs have been built. a focus on eliciting emotionally compelling local  and global situations.
